{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1bc662",
   "metadata": {},
   "source": [
    "# Loading Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95affa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import process_tweet, lookup\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f86da5",
   "metadata": {},
   "source": [
    "### Helping Function For Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fbafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "import numpy as np # Library for linear algebra and math utils\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def test_lookup(func):\n",
    "    freqs = {('sad', 0): 4,\n",
    "             ('happy', 1): 12,\n",
    "             ('oppressed', 0): 7}\n",
    "    word = 'happy'\n",
    "    label = 1\n",
    "    if func(freqs, word, label) == 12:\n",
    "        return 'SUCCESS!!'\n",
    "    return 'Failed Sanity Check!'\n",
    "\n",
    "\n",
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n\n",
    "\n",
    "# From: https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py\n",
    "\n",
    "\n",
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of `x` and `y`\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array_like, shape (n, )\n",
    "        Input data.\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    Other parameters\n",
    "    ----------------\n",
    "    kwargs : `~matplotlib.patches.Patch` properties\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0),\n",
    "                      width=ell_radius_x * 2,\n",
    "                      height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor,\n",
    "                      **kwargs)\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47702ca",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5393cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e501a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fc4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eaaf4",
   "metadata": {},
   "source": [
    "# Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c2f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at_tweet</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wed Jul 03 07:20:15 +0000 2019</td>\n",
       "      <td>ÿ™€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÖ€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ‚ù§Ô∏è  ⁄©€åÿß...</td>\n",
       "      <td>226</td>\n",
       "      <td>1097</td>\n",
       "      <td>127</td>\n",
       "      <td>Punjab, Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fri Jul 02 02:50:53 +0000 2021</td>\n",
       "      <td>Happy birthday to my brother n boss , May you ...</td>\n",
       "      <td>54</td>\n",
       "      <td>273</td>\n",
       "      <td>9</td>\n",
       "      <td>Punjab, Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fri Jul 02 12:12:53 +0000 2021</td>\n",
       "      <td>‚ù§Ô∏è‚ù§Ô∏è</td>\n",
       "      <td>50</td>\n",
       "      <td>131</td>\n",
       "      <td>13</td>\n",
       "      <td>Punjab, Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tue Oct 15 22:29:16 +0000 2019</td>\n",
       "      <td>`suspicious ¬∞jikook au jimin'in ya≈üadƒ±ƒüƒ± kasab...</td>\n",
       "      <td>522</td>\n",
       "      <td>2134</td>\n",
       "      <td>86</td>\n",
       "      <td>ƒ∞stanbul, T√ºrkiye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wed May 13 04:34:31 +0000 2020</td>\n",
       "      <td>Speaking of @SpiderMan... üòÇ   https://t.co/uGJ...</td>\n",
       "      <td>74</td>\n",
       "      <td>535</td>\n",
       "      <td>11</td>\n",
       "      <td>Follow the ATP Tour ‚û°</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                created_at_tweet  \\\n",
       "0           0  Wed Jul 03 07:20:15 +0000 2019   \n",
       "1           1  Fri Jul 02 02:50:53 +0000 2021   \n",
       "2           2  Fri Jul 02 12:12:53 +0000 2021   \n",
       "3           3  Tue Oct 15 22:29:16 +0000 2019   \n",
       "4           4  Wed May 13 04:34:31 +0000 2020   \n",
       "\n",
       "                                           full_text retweet_count  \\\n",
       "0  ÿ™€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÖ€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ‚ù§Ô∏è  ⁄©€åÿß...           226   \n",
       "1  Happy birthday to my brother n boss , May you ...            54   \n",
       "2                                               ‚ù§Ô∏è‚ù§Ô∏è            50   \n",
       "3  `suspicious ¬∞jikook au jimin'in ya≈üadƒ±ƒüƒ± kasab...           522   \n",
       "4  Speaking of @SpiderMan... üòÇ   https://t.co/uGJ...            74   \n",
       "\n",
       "  favorite_count reply_count               location  \n",
       "0           1097         127       Punjab, Pakistan  \n",
       "1            273           9       Punjab, Pakistan  \n",
       "2            131          13       Punjab, Pakistan  \n",
       "3           2134          86      ƒ∞stanbul, T√ºrkiye  \n",
       "4            535          11  Follow the ATP Tour ‚û°  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1=pd.read_csv('Random Tweets from Pakistan- Cleaned- Anonymous.csv')\n",
    "D1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db370a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=D1['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded10728",
   "metadata": {},
   "source": [
    "# Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24c7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=D1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a42a30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ÿ™€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÖ€åÿ±ÿß ŸÑ€å⁄àÿ± ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ ŸÜŸàÿßÿ≤ ÿ¥ÿ±€åŸÅ‚ù§Ô∏è  ⁄©€åÿß...\n",
       "1     Happy birthday to my brother n boss , May you ...\n",
       "2                                                  ‚ù§Ô∏è‚ù§Ô∏è\n",
       "3     `suspicious ¬∞jikook au jimin'in ya≈üadƒ±ƒüƒ± kasab...\n",
       "4     Speaking of @SpiderMan... üòÇ   https://t.co/uGJ...\n",
       "                            ...                        \n",
       "95                            @aishaesthetic_ Ha na üî•ü•∫üòÇ\n",
       "96    As a retd govt servant and someone above 65, b...\n",
       "97    ÿÆÿß⁄© ŸÖÿ¨⁄æ ŸÖ€å⁄∫ ⁄©ŸÖÿßŸÑ ÿ±⁄©⁄æÿß ⁄æ€í ÿß€í ÿÆÿØÿß ÿ™Ÿà ŸÜ€í ÿ≥ŸÜÿ®⁄æÿßŸÑ ÿ±...\n",
       "98    The hype they created for osman was epic but b...\n",
       "99    210624 owhat fansign with #KUN ‚ù§Ô∏è  Kun imitate...\n",
       "Name: full_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62c333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0f8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['psycholog', 'say', 'girl', 'cut', 'long', 'hair', '...', 'kill', 'man', '√∞√ø', '‚Äò', '‚Ç¨']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_tweet = \"Psychology says; The girl who can cut her long hair... Can kill a man too.√∞≈∏‚Äò‚Ç¨\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21329aaa",
   "metadata": {},
   "source": [
    "# Count Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519e2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word,y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1ea2c",
   "metadata": {},
   "source": [
    "Apply Count Function on Test data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3715f095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ÿ™€åÿ±ÿß', 1): 1,\n",
       " ('ŸÑ€å⁄àÿ±', 1): 2,\n",
       " ('ŸÖ€åÿ±ÿß', 1): 1,\n",
       " ('ŸÜŸàÿßÿ≤', 1): 2,\n",
       " ('ÿ¥ÿ±€åŸÅ', 1): 2,\n",
       " ('‚ù§', 1): 1,\n",
       " ('Ô∏è', 1): 1,\n",
       " ('⁄©€åÿß', 1): 1,\n",
       " ('ÿ¢Ÿæ⁄©ÿß', 1): 1,\n",
       " ('ÿ®⁄æ€å', 1): 1,\n",
       " ('⁄æ€í', 1): 1,\n",
       " ('ÿü', 1): 3,\n",
       " ('happi', 0): 1,\n",
       " ('birthday', 0): 1,\n",
       " ('brother', 0): 1,\n",
       " ('n', 0): 1,\n",
       " ('boss', 0): 1,\n",
       " ('may', 0): 1,\n",
       " ('mani', 0): 2,\n",
       " ('üéÇ', 0): 3,\n",
       " ('gem', 0): 1,\n",
       " ('üíé', 0): 1,\n",
       " ('happybirthdayatifrauf', 0): 1,\n",
       " ('‚ù§', 0): 2,\n",
       " ('Ô∏è', 0): 2,\n",
       " ('suspici', 0): 1,\n",
       " ('¬∞', 0): 1,\n",
       " ('jikook', 0): 1,\n",
       " ('au', 0): 1,\n",
       " (\"jimin'in\", 0): 1,\n",
       " ('ya≈üadƒ±ƒüƒ±', 0): 1,\n",
       " ('kasabada', 0): 1,\n",
       " ('aniden', 0): 1,\n",
       " ('ba≈ülayan', 0): 1,\n",
       " ('cinayetl', 0): 1,\n",
       " ('onun', 0): 1,\n",
       " ('kendisini', 0): 1,\n",
       " ('s√ºrekli', 0): 1,\n",
       " ('takip', 0): 1,\n",
       " ('eden', 0): 1,\n",
       " ('yeni', 0): 1,\n",
       " ('kom≈üusu', 0): 1,\n",
       " (\"jungkook'tan\", 0): 1,\n",
       " ('≈ü√ºphelenmesin', 0): 1,\n",
       " ('neden', 0): 1,\n",
       " ('olur', 0): 1,\n",
       " ('speak', 0): 1,\n",
       " ('...', 0): 1,\n",
       " ('üòÇ', 0): 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Testing your function\n",
    "\n",
    "\n",
    "result = {}\n",
    "tweets = D1[0:100]\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09fabf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49699ee9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7d0ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "        # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48e25ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c20a0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a1e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.5737795839220972\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Experiment with your own tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03450f8d",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a586cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680abf4",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31728917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9940\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfcf8d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 2.15\n",
      "I am bad -> -1.29\n",
      "this movie should have been great. -> 2.14\n",
      "great -> 2.14\n",
      "great great -> 4.28\n",
      "great great great -> 6.41\n",
      "great great great great -> 8.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8edb2d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.802119197347832"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f74958bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "        word: string to lookup\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs,word,1)\n",
    "\n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs,word,0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    ### END CODE HERE ###\n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02aa142b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 161, 'negative': 18, 'ratio': 8.526315789473685}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'happi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed62e0",
   "metadata": {},
   "source": [
    "# Defining Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55ae6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    word_list = {}\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3261044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':(': {'positive': 1, 'negative': 3663, 'ratio': 0.0005458515283842794},\n",
       " ':-(': {'positive': 0, 'negative': 378, 'ratio': 0.002638522427440633},\n",
       " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '26': {'positive': 0, 'negative': 20, 'ratio': 0.047619047619047616},\n",
       " '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728},\n",
       " 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
       " '‚ôõ': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " '„Äã': {'positive': 0, 'negative': 210, 'ratio': 0.004739336492890996},\n",
       " 'beliÃáev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'wiÃáll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'justiÃán': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ÔΩìÔΩÖÔΩÖ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
       " 'ÔΩçÔΩÖ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function: find negative words at or below a threshold\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0783f05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
       " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
       " ':)': {'positive': 2847, 'negative': 2, 'ratio': 949.3333333333334},\n",
       " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':d': {'positive': 498, 'negative': 0, 'ratio': 499.0},\n",
       " ':p': {'positive': 104, 'negative': 0, 'ratio': 105.0},\n",
       " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
       " ':-)': {'positive': 543, 'negative': 0, 'ratio': 544.0},\n",
       " \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
       " 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
       " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
       " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
       " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
       " 'via': {'positive': 60, 'negative': 1, 'ratio': 30.5},\n",
       " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
       " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
       " 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
       " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
       " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Test your function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3655b581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'whatev stil l young >:-('\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0037dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1483210220741955\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'i am happy'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)\n",
    "if p > 0:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32268d84",
   "metadata": {},
   "source": [
    "So for Further this tis array in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2033a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
